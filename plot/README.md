# This folder contains figures generated by the repository

## Visualize DTox visible neural network model training, performance, and comparison with other methods   

+ First, the evolution of training/testing loss over epoches during DTox learning process was visualized using [line charts](compound_target_probability_tox21_implementation/training_loss/) for each dataset. Overall, the optimal DTox model can be reached within 100-200 epoches for the implementation task on Tox21 datasets. After the optimal point, training loss keeps decreasing while testing loss remains fluctuant or increases.    

+ Next, model statistics of optimal DTox and matched matched multi-layer perceptron (MLP) were visualized using [barplots](compound_target_probability_tox21_implementation/parameter_comparison/) for each dataset. On average, an optimal DTox visible neural network model contains ~400 hidden pathway modules and ~40,000 learned weight parameter. The average ratio between training samples and number of DTox parameters is ~0.15, five times of the average ratio for VNN parameters.

+ Then, normalized DTox model performance of Tox21 datasets across root pathway settings were visualized using [heatmap](compound_target_probability_tox21_implementation/hyperparameter_comparison/compound_target_probability_tox21_implementation_rt_training_root_loss_normalized_comparison_by_dataset.pdf) and [upsetplot](compound_target_probability_tox21_implementation/hyperparameter_comparison/compound_target_probability_tox21_implementation_rt_upset.pdf). The optimal root pathway setting varies by dataset. Overall, including more pathways in the root of DTox visible neural network results in better model performance. 

+ Finally, validation performance of DTox visible neural network model (VNN) was compared against other method implementations. The comparison was visualized using [barplots](compound_target_probability_tox21_implementation/method_comparison/) for each dataset. Four other method implementations were considered: i) alternative DTox visible neural network model with the same setting as optimal model but built under shuffled Reactome pathway hierarchy (VNN-S), ii) fully connected MLP neural network model, otherwise with the same number of hidden layer/neuron as the matched DTox model (MLP), iii) optimal random forest model after hyperparameter tuning (RF), and iv) optimal gradient boosting model after hyperparameter tuning (GB).   
